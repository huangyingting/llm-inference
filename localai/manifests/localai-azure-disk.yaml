apiVersion: v1
kind: Namespace
metadata:
  name: llm
  annotations:
    scheduler.alpha.kubernetes.io/defaultTolerations: '[{"Key": "kubernetes.azure.com/scalesetpriority", "Operator": "Equal", "Value": "spot", "Effect": "NoSchedule"}, {"Key": "sku", "Operator": "Equal", "Value": "gpu", "Effect": "NoSchedule"}]'
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: localai
  name: localai
  namespace: llm
spec:
  selector:
    matchLabels:
      app: localai
  serviceName: localai
  replicas: 2
  template:
    metadata:
      labels:
        app: localai
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: "app"
                  operator: In
                  values:
                  - localai
            topologyKey: "kubernetes.io/hostname"    
      containers:
      - image: ghcr.io/huangyingting/llm-inference-localai:main
        imagePullPolicy: Always
        name: localai
        resources:
          limits:
           nvidia.com/gpu: 1        
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP        
        volumeMounts:
        - name: shm
          mountPath: /dev/shm        
        - name: localai
          mountPath: "/data"
      volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 1Gi
  volumeClaimTemplates:
  - metadata:
      name: localai
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: managed-csi-premium
      resources:
        requests:
          storage: 16Gi      
---
apiVersion: v1
kind: Service
metadata:
  name: localai
  namespace: llm
  labels:
    app: localai
spec:
  ports:
  - port: 8080
    targetPort: 8000
  selector:
    app: localai